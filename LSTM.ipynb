{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9gX0ZBW392x"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4ZqXF604IGE",
        "outputId": "c4838919-ae68-4e06-9589-b9d20e835fe3"
      },
      "source": [
        "data_file = open(\"dataset.txt\", \"r\")\n",
        "lines = data_file.readlines()\n",
        "\n",
        "letter_to_idx = {}\n",
        "idx_to_letter = {}\n",
        "cnt = 0\n",
        "for i in range(10):\n",
        "    letter_to_idx[str(i)] = len(letter_to_idx)\n",
        "for line in lines:\n",
        "    if \"sin\" in line or \"cos\" in line or \"tan\" in line:\n",
        "        line = line.replace(\"sin\", \"!\")\n",
        "        line = line.replace(\"cos\", \"@\")\n",
        "        line = line.replace(\"tan\", \"#\")\n",
        "        cnt += 1\n",
        "    \n",
        "    for letter in line:\n",
        "        if letter.isalpha(): letter = \"$\"\n",
        "        if letter not in letter_to_idx:\n",
        "            letter_to_idx[letter] = len(letter_to_idx)\n",
        "\n",
        "for k,v in letter_to_idx.items():\n",
        "    idx_to_letter[v] = k\n",
        "random.shuffle(lines)\n",
        "train_lines = lines[: int(len(lines)*0.97) ]\n",
        "test_lines = lines[int(len(lines)*0.97): ]\n",
        "print(len(train_lines),len(test_lines))\n",
        "print(cnt)\n",
        "print(letter_to_idx)\n",
        "print(idx_to_letter)\n",
        "print(len(lines))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "970000 30000\n",
            "30095\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '(': 10, '-': 11, '*': 12, '$': 13, ')': 14, '=': 15, '\\n': 16, '+': 17, '@': 18, '#': 19, '!': 20}\n",
            "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '(', 11: '-', 12: '*', 13: '$', 14: ')', 15: '=', 16: '\\n', 17: '+', 18: '@', 19: '#', 20: '!'}\n",
            "1000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGqg3Ccd4KZB",
        "outputId": "16b0801c-0846-49b8-9657-e067e934238b"
      },
      "source": [
        "tags = []\n",
        "max_digit = 0\n",
        "for line in lines:\n",
        "    if \"sin\" in line or \"cos\" in line or \"tan\" in line:\n",
        "        line = line.replace(\"sin\", \"!\")\n",
        "        line = line.replace(\"cos\", \"@\")\n",
        "        line = line.replace(\"tan\", \"#\")\n",
        "        cnt += 1\n",
        "    tag_line = []\n",
        "    for i, letter in enumerate(line):\n",
        "        # print(letter, letter.isnumeric())\n",
        "        if letter.isnumeric():\n",
        "            j = i; digit = 0\n",
        "            while j < len(line):\n",
        "                if line[j].isnumeric(): \n",
        "                    digit += 1\n",
        "                    if digit > max_digit: max_digit = digit\n",
        "                else: break\n",
        "                j += 1\n",
        "            tag_line.append(digit)\n",
        "        else: tag_line.append(0)\n",
        "    tags.append(tag_line)\n",
        "train_tags = tags[: int(len(tags)*0.97) ]\n",
        "test_tags = tags[int(len(tags)*0.97) : ]\n",
        "print(len(train_tags),len(test_tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "970000 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuD-3LNq5Ly0",
        "outputId": "1abdfdcd-8d2d-42d1-b786-b6fc3df3807f"
      },
      "source": [
        "print(max_digit)\n",
        "print(len(tags))\n",
        "print(len(tags[100]),len(lines[100]))\n",
        "for a,b in zip(test_lines, test_tags):\n",
        "    if \"sin\" in a or \"cos\" in a or \"tan\" in a:\n",
        "        a = a.replace(\"sin\", \"!\")\n",
        "        a = a.replace(\"cos\", \"@\")\n",
        "        a = a.replace(\"tan\", \"#\")\n",
        "    if len(a) != len(b): print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "1000000\n",
            "14 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAxeC3z75TJe"
      },
      "source": [
        "class LSTMPoly(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tag_size, tag_em_dim, att_dim):\n",
        "        super(LSTMPoly, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.tag_embeddings = nn.Embedding(tag_size, tag_em_dim)\n",
        "        self.att_embeddings = nn.Embedding(10+1, att_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim + tag_em_dim, hidden_dim, num_layers=4, batch_first=True)\n",
        "        self.classifier1 = nn.Linear(hidden_dim * 3 , vocab_size)\n",
        "        # self.activation = nn.ReLU()\n",
        "        # self.classifier2 = nn.Linear(64, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, tags):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        tag_ems = self.tag_embeddings(tags)\n",
        "        overall_embeds = torch.cat((embeds,tag_ems), dim=1)\n",
        "        overall_embeds = overall_embeds.view(1, len(inputs), -1)\n",
        "        lstm_out, _ = self.lstm(overall_embeds)\n",
        "        lstm_out = lstm_out.squeeze(0)\n",
        "        \n",
        "        # embeds = embeds.view(1, len(inputs), -1)\n",
        "\n",
        "        att_embeds = self.att_embeddings.weight\n",
        "        # print(lstm_out.shape, att_embeds.shape)\n",
        "        attention_logits = torch.matmul(lstm_out, att_embeds.t())\n",
        "        attention_scores = self.softmax(attention_logits)\n",
        "        \n",
        "        attention_embeds = torch.matmul(attention_scores, att_embeds)\n",
        "        attention_embeds = torch.div(attention_embeds, 10+1) #torch.Size([36, 150])\n",
        "        # print(lstm_out.shape, attention_embeds.shape)\n",
        "        # print((lstm_out * attention_embeds).shape)\n",
        "        attended_embeds = torch.cat((lstm_out,attention_embeds,lstm_out * attention_embeds), dim=1)\n",
        "\n",
        "        # print(attended_embeds.shape)\n",
        "        # attended_embeds = attended_embeds.view(1, len(inputs), -1)\n",
        "\n",
        "        # lstm_out, _ = self.lstm(overall_embeds)\n",
        "        # print(overall_embeds.shape, lstm_out.shape)\n",
        "        logits = self.classifier1(attended_embeds)\n",
        "        # logits = self.activation(logits)\n",
        "        # logits = self.classifier2(logits)\n",
        "        logits = logits.unsqueeze(0)\n",
        "        return logits, attention_logits\n",
        "\n",
        "def line_2_tokens(line, dic):\n",
        "    tokens = [ dic[letter] if not letter.isalpha() else dic[\"$\"] for letter in line]\n",
        "    return tokens\n",
        "\n",
        "def tokens_2_lines(line, dic):\n",
        "    tokens = [ dic[letter] for letter in line]\n",
        "    return tokens\n",
        "\n",
        "def line_2_att_token(line):\n",
        "    tokens = [ int(letter) if letter.isnumeric() else 10 for letter in line]\n",
        "    return tokens\n",
        "cnt_10 = 0; cnt_num = 0;\n",
        "def evaluate_dumb(model, test_lines, test_tags):\n",
        "    # cnt_10 = 0; cnt_num = 0; cnt_10_correct=0; cnt_num_correct=0\n",
        "    total_letters = 0\n",
        "    correct_letters = 0\n",
        "    total_sentence = 0\n",
        "    correct_sentence = 0\n",
        "    att_correct = 0\n",
        "    att_total = 0\n",
        "    soft = nn.Softmax(dim=-1)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (line, line_tags) in enumerate(zip(test_lines,test_tags)):\n",
        "            \n",
        "            if \"sin\" in line or \"cos\" in line or \"tan\" in line:\n",
        "                line = line.replace(\"sin\", \"!\")\n",
        "                line = line.replace(\"cos\", \"@\")\n",
        "                line = line.replace(\"tan\", \"#\")\n",
        "            target_len = len(line.split(\"=\")[1])\n",
        "            tokens = line_2_tokens(line, letter_to_idx)\n",
        "            \n",
        "            train_tokens = tokens[:-1]; target_tokens = tokens[-target_len:]; line_tags = line_tags[:-1]\n",
        "            att_target = tokens_2_lines(target_tokens, idx_to_letter)\n",
        "            att_target = line_2_att_token(att_target)\n",
        "\n",
        "            train_tokens = torch.tensor(train_tokens).to(device);# target_tokens = torch.tensor(target_tokens)\n",
        "            line_tags = torch.tensor(line_tags).to(device)\n",
        "\n",
        "            logits, att_logits = model.forward(train_tokens, line_tags)\n",
        "            preds = soft(logits)\n",
        "            att_preds = soft(att_logits)\n",
        "            # atts = soft(att_logits)\n",
        "            preds_target = preds[:,-target_len:,:]\n",
        "            att_preds_target = att_preds[-target_len:,:]\n",
        "            # print(len(target_tokens))\n",
        "            preds_target = np.argmax(preds_target.cpu().detach().numpy(), axis=-1)[0]\n",
        "            att_preds_target = np.argmax(att_preds_target.cpu().detach().numpy(), axis=-1)\n",
        "\n",
        "            # print(line.split(\"=\")[0])\n",
        "            # print(tokens_2_lines(preds_target, idx_to_letter) )\n",
        "            # print(tokens_2_lines(target_tokens, idx_to_letter) )\n",
        "            # print(\" \")\n",
        "\n",
        "            # if i == 20: break\n",
        "\n",
        "            sentence_correct = True\n",
        "            # print(tokens_2_lines(preds_target, idx_to_letter))\n",
        "            # print(tokens_2_lines(target_tokens, idx_to_letter))\n",
        "            # print(\" \")\n",
        "            for p, t in zip(preds_target, target_tokens):\n",
        "                if p == t: correct_letters += 1\n",
        "                else: sentence_correct = False\n",
        "                total_letters += 1\n",
        "            total_sentence += 1\n",
        "            if sentence_correct: correct_sentence += 1\n",
        "\n",
        "            for p, t in zip(att_preds_target, att_target):\n",
        "                if p == t: att_correct += 1\n",
        "                # if p == 10: \n",
        "                #     cnt_10 += 1\n",
        "                #     if p == t: cnt_10_correct += 1\n",
        "                # else: \n",
        "                #     cnt_num += 1\n",
        "                #     if p == t: cnt_num_correct += 1\n",
        "            att_total += len(att_target)\n",
        "\n",
        "            # print(att_preds_target)\n",
        "            # print(att_target)\n",
        "            # print( )\n",
        "\n",
        "    # print(cnt_10, cnt_10_correct, cnt_num, cnt_num_correct)\n",
        "    return total_letters, correct_letters, total_sentence, correct_sentence, att_correct, att_total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "708rDq10Np3f",
        "outputId": "936f33fa-ef58-4d37-a3f8-a5f85c670d5e"
      },
      "source": [
        "evaluate_dumb(model, test_lines, test_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20-8*k)*(7*k+7)\n",
            "['-', '5', '6', '*', '$', '*', '*', '2', '+', '8', '4', '*', '$', '+', '1', '4', '0', '\\n']\n",
            "['-', '5', '6', '*', '$', '*', '*', '2', '+', '8', '4', '*', '$', '+', '1', '4', '0', '\\n']\n",
            " \n",
            "(14-6*z)*(27-z)\n",
            "['6', '*', '$', '*', '*', '2', '-', '1', '7', '2', '*', '$', '+', '3', '7', '8', '\\n']\n",
            "['6', '*', '$', '*', '*', '2', '-', '1', '7', '6', '*', '$', '+', '3', '7', '8', '\\n']\n",
            " \n",
            "(-7*a-7)*(a+16)\n",
            "['-', '7', '*', '$', '*', '*', '2', '-', '1', '1', '5', '*', '$', '-', '1', '1', '2', '\\n']\n",
            "['-', '7', '*', '$', '*', '*', '2', '-', '1', '1', '9', '*', '$', '-', '1', '1', '2', '\\n']\n",
            " \n",
            "(x+23)*(2*x-22)\n",
            "['2', '*', '$', '*', '*', '2', '+', '2', '4', '*', '$', '-', '5', '0', '6', '\\n']\n",
            "['2', '*', '$', '*', '*', '2', '+', '2', '4', '*', '$', '-', '5', '0', '6', '\\n']\n",
            " \n",
            "7*i*(i-20)\n",
            "['7', '*', '$', '*', '*', '2', '-', '1', '4', '0', '*', '$', '\\n']\n",
            "['7', '*', '$', '*', '*', '2', '-', '1', '4', '0', '*', '$', '\\n']\n",
            " \n",
            "(h-13)*(8*h+28)\n",
            "['8', '*', '$', '*', '*', '2', '-', '7', '6', '*', '$', '-', '3', '6', '4', '\\n']\n",
            "['8', '*', '$', '*', '*', '2', '-', '7', '6', '*', '$', '-', '3', '6', '4', '\\n']\n",
            " \n",
            "-4*z*(-z-2)\n",
            "['4', '*', '$', '*', '*', '2', '+', '8', '*', '$', '\\n']\n",
            "['4', '*', '$', '*', '*', '2', '+', '8', '*', '$', '\\n']\n",
            " \n",
            "18*s**2\n",
            "['1', '8', '*', '$', '*', '*', '2', '\\n']\n",
            "['1', '8', '*', '$', '*', '*', '2', '\\n']\n",
            " \n",
            "-5*h*(-8*h-6)\n",
            "['4', '0', '*', '$', '*', '*', '2', '+', '3', '0', '*', '$', '\\n']\n",
            "['4', '0', '*', '$', '*', '*', '2', '+', '3', '0', '*', '$', '\\n']\n",
            " \n",
            "(t+12)*(7*t-32)\n",
            "['7', '*', '$', '*', '*', '2', '+', '4', '2', '*', '$', '-', '3', '8', '4', '\\n']\n",
            "['7', '*', '$', '*', '*', '2', '+', '5', '2', '*', '$', '-', '3', '8', '4', '\\n']\n",
            " \n",
            "-15*h**2\n",
            "['-', '1', '5', '*', '$', '*', '*', '2', '\\n']\n",
            "['-', '1', '5', '*', '$', '*', '*', '2', '\\n']\n",
            " \n",
            "(j-6)*(2*j+29)\n",
            "['2', '*', '$', '*', '*', '2', '+', '1', '5', '*', '$', '-', '1', '7', '4', '\\n']\n",
            "['2', '*', '$', '*', '*', '2', '+', '1', '7', '*', '$', '-', '1', '7', '4', '\\n']\n",
            " \n",
            "-6*x*(4*x-31)\n",
            "['-', '2', '4', '*', '$', '*', '*', '2', '+', '1', '8', '6', '*', '$', '\\n']\n",
            "['-', '2', '4', '*', '$', '*', '*', '2', '+', '1', '8', '6', '*', '$', '\\n']\n",
            " \n",
            "-s*(4*s+19)\n",
            "['-', '4', '*', '$', '*', '*', '2', '-', '1', '9', '*', '$', '\\n']\n",
            "['-', '4', '*', '$', '*', '*', '2', '-', '1', '9', '*', '$', '\\n']\n",
            " \n",
            "(21-7*h)*(30-7*h)\n",
            "['4', '9', '*', '$', '*', '*', '2', '-', '3', '4', '1', '*', '$', '+', '6', '3', '0', '\\n']\n",
            "['4', '9', '*', '$', '*', '*', '2', '-', '3', '5', '7', '*', '$', '+', '6', '3', '0', '\\n']\n",
            " \n",
            "2*s*(-9*s-7)\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '-', '1', '4', '*', '$', '\\n']\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '-', '1', '4', '*', '$', '\\n']\n",
            " \n",
            "-s*(6*s-12)\n",
            "['-', '6', '*', '$', '*', '*', '2', '+', '1', '2', '*', '$', '\\n']\n",
            "['-', '6', '*', '$', '*', '*', '2', '+', '1', '2', '*', '$', '\\n']\n",
            " \n",
            "(s-17)*(2*s-3)\n",
            "['2', '*', '$', '*', '*', '2', '-', '3', '3', '*', '$', '+', '5', '1', '\\n']\n",
            "['2', '*', '$', '*', '*', '2', '-', '3', '7', '*', '$', '+', '5', '1', '\\n']\n",
            " \n",
            "(6-7*#(i))*(#(i)-7)\n",
            "['-', '7', '*', '!', '(', '$', ')', '*', '*', '2', '+', '5', '5', '*', '!', '(', '$', ')', '-', '4', '2', '\\n']\n",
            "['-', '7', '*', '#', '(', '$', ')', '*', '*', '2', '+', '5', '5', '*', '#', '(', '$', ')', '-', '4', '2', '\\n']\n",
            " \n",
            "(-t-13)*(5*t-31)\n",
            "['-', '5', '*', '$', '*', '*', '2', '-', '3', '2', '*', '$', '+', '4', '0', '3', '\\n']\n",
            "['-', '5', '*', '$', '*', '*', '2', '-', '3', '4', '*', '$', '+', '4', '0', '3', '\\n']\n",
            " \n",
            "-18*t**2\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '\\n']\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '\\n']\n",
            " \n",
            "(2-h)*(5*h+21)\n",
            "['-', '5', '*', '$', '*', '*', '2', '-', '1', '9', '*', '$', '+', '4', '2', '\\n']\n",
            "['-', '5', '*', '$', '*', '*', '2', '-', '1', '1', '*', '$', '+', '4', '2', '\\n']\n",
            " \n",
            "(3*o+18)*(5*o+23)\n",
            "['1', '5', '*', '$', '*', '*', '2', '+', '1', '5', '1', '*', '$', '+', '4', '1', '4', '\\n']\n",
            "['1', '5', '*', '$', '*', '*', '2', '+', '1', '5', '9', '*', '$', '+', '4', '1', '4', '\\n']\n",
            " \n",
            "(25-4*@(n))*(5*@(n)+31)\n",
            "['-', '2', '0', '*', '!', '(', '$', ')', '*', '*', '2', '+', '1', '(', '$', ')', '+', '7', '7', '5', '\\n']\n",
            "['-', '2', '0', '*', '@', '(', '$', ')', '*', '*', '2', '+', '@', '(', '$', ')', '+', '7', '7', '5', '\\n']\n",
            " \n",
            "(-3*a-23)*(6*a-5)\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '-', '1', '1', '1', '*', '$', '+', '1', '1', '5', '\\n']\n",
            "['-', '1', '8', '*', '$', '*', '*', '2', '-', '1', '2', '3', '*', '$', '+', '1', '1', '5', '\\n']\n",
            " \n",
            "(x-17)*(4*x+7)\n",
            "['4', '*', '$', '*', '*', '2', '-', '6', '1', '*', '$', '-', '1', '1', '9', '\\n']\n",
            "['4', '*', '$', '*', '*', '2', '-', '6', '1', '*', '$', '-', '1', '1', '9', '\\n']\n",
            " \n",
            "(19-3*n)*(7*n+29)\n",
            "['-', '2', '1', '*', '$', '*', '*', '2', '+', '4', '0', '*', '$', '+', '5', '5', '1', '\\n']\n",
            "['-', '2', '1', '*', '$', '*', '*', '2', '+', '4', '6', '*', '$', '+', '5', '5', '1', '\\n']\n",
            " \n",
            "-5*s*(-5*s-30)\n",
            "['2', '5', '*', '$', '*', '*', '2', '+', '1', '5', '0', '*', '$', '\\n']\n",
            "['2', '5', '*', '$', '*', '*', '2', '+', '1', '5', '0', '*', '$', '\\n']\n",
            " \n",
            "-5*s*(25-5*s)\n",
            "['2', '5', '*', '$', '*', '*', '2', '-', '1', '2', '5', '*', '$', '\\n']\n",
            "['2', '5', '*', '$', '*', '*', '2', '-', '1', '2', '5', '*', '$', '\\n']\n",
            " \n",
            "(-4*s-26)*(-s-29)\n",
            "['4', '*', '$', '*', '*', '2', '+', '1', '4', '2', '*', '$', '+', '7', '5', '4', '\\n']\n",
            "['4', '*', '$', '*', '*', '2', '+', '1', '4', '2', '*', '$', '+', '7', '5', '4', '\\n']\n",
            " \n",
            "-9*a*(4*a+23)\n",
            "['-', '3', '6', '*', '$', '*', '*', '2', '-', '2', '0', '7', '*', '$', '\\n']\n",
            "['-', '3', '6', '*', '$', '*', '*', '2', '-', '2', '0', '7', '*', '$', '\\n']\n",
            " \n",
            "(7*c-4)*(8*c+13)\n",
            "['5', '6', '*', '$', '*', '*', '2', '+', '4', '5', '*', '$', '-', '5', '2', '\\n']\n",
            "['5', '6', '*', '$', '*', '*', '2', '+', '5', '9', '*', '$', '-', '5', '2', '\\n']\n",
            " \n",
            "-8*i*(i-17)\n",
            "['-', '8', '*', '$', '*', '*', '2', '+', '1', '3', '6', '*', '$', '\\n']\n",
            "['-', '8', '*', '$', '*', '*', '2', '+', '1', '3', '6', '*', '$', '\\n']\n",
            " \n",
            "(8-2*i)*(i+12)\n",
            "['-', '2', '*', '$', '*', '*', '2', '-', '1', '6', '*', '$', '+', '9', '6', '\\n']\n",
            "['-', '2', '*', '$', '*', '*', '2', '-', '1', '6', '*', '$', '+', '9', '6', '\\n']\n",
            " \n",
            "(7*t-31)*(8*t-30)\n",
            "['5', '6', '*', '$', '*', '*', '2', '-', '4', '1', '8', '*', '$', '+', '9', '3', '0', '\\n']\n",
            "['5', '6', '*', '$', '*', '*', '2', '-', '4', '5', '8', '*', '$', '+', '9', '3', '0', '\\n']\n",
            " \n",
            "(18-i)*(i-12)\n",
            "['-', '$', '*', '*', '2', '+', '3', '0', '*', '$', '-', '2', '1', '6', '\\n']\n",
            "['-', '$', '*', '*', '2', '+', '3', '0', '*', '$', '-', '2', '1', '6', '\\n']\n",
            " \n",
            "-7*s*(6*s+8)\n",
            "['-', '4', '2', '*', '$', '*', '*', '2', '-', '5', '6', '*', '$', '\\n']\n",
            "['-', '4', '2', '*', '$', '*', '*', '2', '-', '5', '6', '*', '$', '\\n']\n",
            " \n",
            "6*y*(12-6*y)\n",
            "['-', '3', '6', '*', '$', '*', '*', '2', '+', '7', '2', '*', '$', '\\n']\n",
            "['-', '3', '6', '*', '$', '*', '*', '2', '+', '7', '2', '*', '$', '\\n']\n",
            " \n",
            "(6-3*n)*(7*n+1)\n",
            "['-', '2', '1', '*', '$', '*', '*', '2', '+', '3', '1', '*', '$', '+', '6', '\\n']\n",
            "['-', '2', '1', '*', '$', '*', '*', '2', '+', '3', '9', '*', '$', '+', '6', '\\n']\n",
            " \n",
            "t*(-8*t-2)\n",
            "['-', '8', '*', '$', '*', '*', '2', '-', '2', '*', '$', '\\n']\n",
            "['-', '8', '*', '$', '*', '*', '2', '-', '2', '*', '$', '\\n']\n",
            " \n",
            "(-6*i-19)*(7*i-25)\n",
            "['-', '4', '2', '*', '$', '*', '*', '2', '+', '1', '1', '*', '$', '+', '4', '7', '5', '\\n']\n",
            "['-', '4', '2', '*', '$', '*', '*', '2', '+', '1', '7', '*', '$', '+', '4', '7', '5', '\\n']\n",
            " \n",
            "-9*s*(s-25)\n",
            "['-', '9', '*', '$', '*', '*', '2', '+', '2', '2', '5', '*', '$', '\\n']\n",
            "['-', '9', '*', '$', '*', '*', '2', '+', '2', '2', '5', '*', '$', '\\n']\n",
            " \n",
            "(-4*i-1)*(4*i+31)\n",
            "['-', '1', '6', '*', '$', '*', '*', '2', '-', '1', '2', '8', '*', '$', '-', '3', '1', '\\n']\n",
            "['-', '1', '6', '*', '$', '*', '*', '2', '-', '1', '2', '8', '*', '$', '-', '3', '1', '\\n']\n",
            " \n",
            "2*s*(5*s+3)\n",
            "['1', '0', '*', '$', '*', '*', '2', '+', '6', '*', '$', '\\n']\n",
            "['1', '0', '*', '$', '*', '*', '2', '+', '6', '*', '$', '\\n']\n",
            " \n",
            "3*s*(21-9*s)\n",
            "['-', '2', '7', '*', '$', '*', '*', '2', '+', '6', '3', '*', '$', '\\n']\n",
            "['-', '2', '7', '*', '$', '*', '*', '2', '+', '6', '3', '*', '$', '\\n']\n",
            " \n",
            "(3*k-25)*(5*k-29)\n",
            "['1', '5', '*', '$', '*', '*', '2', '-', '2', '1', '4', '*', '$', '+', '7', '2', '5', '\\n']\n",
            "['1', '5', '*', '$', '*', '*', '2', '-', '2', '1', '2', '*', '$', '+', '7', '2', '5', '\\n']\n",
            " \n",
            "(-2*y-20)*(5*y-8)\n",
            "['-', '1', '0', '*', '$', '*', '*', '2', '-', '8', '4', '*', '$', '+', '1', '6', '0', '\\n']\n",
            "['-', '1', '0', '*', '$', '*', '*', '2', '-', '8', '4', '*', '$', '+', '1', '6', '0', '\\n']\n",
            " \n",
            "6*i*(-4*i-16)\n",
            "['-', '2', '4', '*', '$', '*', '*', '2', '-', '9', '6', '*', '$', '\\n']\n",
            "['-', '2', '4', '*', '$', '*', '*', '2', '-', '9', '6', '*', '$', '\\n']\n",
            " \n",
            "-6*i*(i+18)\n",
            "['-', '6', '*', '$', '*', '*', '2', '-', '1', '0', '8', '*', '$', '\\n']\n",
            "['-', '6', '*', '$', '*', '*', '2', '-', '1', '0', '8', '*', '$', '\\n']\n",
            " \n",
            "4*z*(z-4)\n",
            "['4', '*', '$', '*', '*', '2', '-', '1', '6', '*', '$', '\\n']\n",
            "['4', '*', '$', '*', '*', '2', '-', '1', '6', '*', '$', '\\n']\n",
            " \n",
            "-k*(k-28)\n",
            "['-', '$', '*', '*', '2', '+', '2', '8', '*', '$', '\\n']\n",
            "['-', '$', '*', '*', '2', '+', '2', '8', '*', '$', '\\n']\n",
            " \n",
            "455 455 307 284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(762, 739, 50, 32, 739, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPndXNhZnWw2",
        "outputId": "4caab024-109c-482d-fae4-cf4f37b49a9d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8790936040870372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyCBpJLc8-U6",
        "outputId": "8c90d176-1f85-4551-8e1a-aa47738907ee"
      },
      "source": [
        "\n",
        "# device = torch.device(\"cuda\")\n",
        "model_test = LSTMPoly(96, 256, len(letter_to_idx), max_digit+1, 64, 256)\n",
        "# model.forward(torch.tensor([0,1]), torch.tensor([0,1]))\n",
        "model_test.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMPoly(\n",
              "  (word_embeddings): Embedding(21, 96)\n",
              "  (tag_embeddings): Embedding(5, 64)\n",
              "  (att_embeddings): Embedding(11, 256)\n",
              "  (softmax): Softmax(dim=-1)\n",
              "  (lstm): LSTM(160, 256, num_layers=4, batch_first=True)\n",
              "  (classifier1): Linear(in_features=512, out_features=64, bias=True)\n",
              "  (activation): ReLU()\n",
              "  (classifier2): Linear(in_features=64, out_features=21, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUWk_S4ZvjQJ",
        "outputId": "585328fe-63a2-4e37-d451-8717b7ea90ae"
      },
      "source": [
        "\n",
        "\n",
        "for i, (line, line_tags) in enumerate(zip(train_lines, train_tags)):\n",
        "    model.train()\n",
        "    if \"sin\" in line or \"cos\" in line or \"tan\" in line:\n",
        "        line = line.replace(\"sin\", \"!\")\n",
        "        line = line.replace(\"cos\", \"@\")\n",
        "        line = line.replace(\"tan\", \"#\")\n",
        "    \n",
        "    tokens = line_2_tokens(line, letter_to_idx)\n",
        "    target_len = len(line.split(\"=\")[1])\n",
        "    \n",
        "    train_tokens = tokens[:-1]; target_tokens = tokens[-target_len:]; line_tags = line_tags[:-1]\n",
        "    att_target = tokens_2_lines(target_tokens, idx_to_letter)\n",
        "    att_target = line_2_att_token(att_target)\n",
        "    att_target = torch.tensor(att_target).to(device)\n",
        "\n",
        "    train_tokens = torch.tensor(train_tokens).to(device); target_tokens = torch.tensor(target_tokens).to(device)\n",
        "    line_tags = torch.tensor(line_tags).to(device)\n",
        "    logits, att_logits = model_test.forward(train_tokens, line_tags)\n",
        "    # preds = soft(logits)\n",
        "            # atts = soft(att_logits)\n",
        "    att_logits = att_logits[-target_len:,:]\n",
        "    # preds_target = preds[:,-target_len:,:]\n",
        "    # print(att_logits.shape, target_tokens.shape)\n",
        "    # print(att_logits.shape, att_target.shape)\n",
        "    a = nn.CrossEntropyLoss()(att_logits, att_target)\n",
        "    # print(a)\n",
        "    \n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 256]) torch.Size([28, 256])\n",
            "torch.Size([28, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JyHYr2iyeUI",
        "outputId": "db57523b-9cba-49d2-c2a8-bb7e969a5bf5"
      },
      "source": [
        "A = torch.rand(2, 3,1)# torch.tensor([[1,0],[0,1]])\n",
        "B = torch.rand(2, 3, 2) #torch.tensor( [ [1,2,3] , [4, 5, 6] ] )\n",
        "# A = A.unsqueeze(1)\n",
        "print(A.shape, B.shape)\n",
        "# C = A * B\n",
        "C = torch.multiply(A,B)\n",
        "print(C.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 1]) torch.Size([1, 3, 2])\n",
            "torch.Size([2, 3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd86RJK36YGV",
        "outputId": "63c71846-6f65-4b72-bd86-870b6bd354ae"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = LSTMPoly(96, 256, len(letter_to_idx), max_digit+1, 64, 256)\n",
        "# model.forward(torch.tensor([0,1]), torch.tensor([0,1]))\n",
        "model.to(device)\n",
        "weights_main = [3.0] * 10 + [1.0] * 11\n",
        "class_weights_main = torch.FloatTensor(weights_main).to(device)\n",
        "loss_function = nn.CrossEntropyLoss(class_weights_main) #nn.NLLLoss()\n",
        "weights = [1.0] * 10 + [0.06]\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "att_loss_func = nn.CrossEntropyLoss(class_weights) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for i, (line, line_tags) in enumerate(zip(train_lines, train_tags)):\n",
        "    model.train()\n",
        "    if \"sin\" in line or \"cos\" in line or \"tan\" in line:\n",
        "        line = line.replace(\"sin\", \"!\")\n",
        "        line = line.replace(\"cos\", \"@\")\n",
        "        line = line.replace(\"tan\", \"#\")\n",
        "    \n",
        "    tokens = line_2_tokens(line, letter_to_idx)\n",
        "    target_len = len(line.split(\"=\")[1])\n",
        "    \n",
        "    train_tokens = tokens[:-1]; target_tokens = tokens[-target_len:]; line_tags = line_tags[:-1]\n",
        "    att_target = tokens_2_lines(target_tokens, idx_to_letter)\n",
        "    att_target = line_2_att_token(att_target)\n",
        "    \n",
        "\n",
        "    train_tokens = torch.tensor(train_tokens).to(device); target_tokens = torch.tensor(target_tokens).to(device)\n",
        "    line_tags = torch.tensor(line_tags).to(device); att_target = torch.tensor(att_target).to(device)\n",
        "\n",
        "    logits, att_logits = model.forward(train_tokens, line_tags)\n",
        "    logits = logits[:,-target_len:,:]\n",
        "    att_logits = att_logits[-target_len:,:]\n",
        "    loss = loss_function(logits.squeeze(0), target_tokens.squeeze(0))\n",
        "    att_loss = att_loss_func(att_logits, att_target)\n",
        "\n",
        "    total_loss = loss + 0.1 * att_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 60000 == 0: \n",
        "        total_letters, correct_letters, total_sentence, correct_sentence, att_correct, att_total = evaluate_dumb(model, test_lines, test_tags)\n",
        "        print(i, total_letters, correct_letters, correct_letters/total_letters, total_sentence, correct_sentence, correct_sentence/total_sentence, att_correct/att_total )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 459606 13959 0.03037166616623804 30000 0 0.0 0.11919122030608825\n",
            "60000 459606 375822 0.8177047297032676 30000 4863 0.1621 0.8626780329238521\n",
            "120000 459606 412493 0.8974926349960618 30000 10205 0.3401666666666667 0.9183169932507409\n",
            "180000 459606 428124 0.9315021997101866 30000 12035 0.40116666666666667 0.9367110089946606\n",
            "240000 459606 433961 0.9442022079781378 30000 13319 0.4439666666666667 0.9475746617755207\n",
            "300000 459606 436570 0.9498788092409586 30000 14207 0.4735666666666667 0.9530532673637855\n",
            "360000 459606 437307 0.951482356627198 30000 14465 0.4821666666666667 0.9549831812465459\n",
            "420000 459606 438904 0.9549570719268243 30000 15337 0.5112333333333333 0.957831272872852\n",
            "480000 459606 440133 0.9576311014216524 30000 15856 0.5285333333333333 0.9596676283599431\n",
            "540000 459606 441095 0.9597241985526733 30000 16370 0.5456666666666666 0.9626462665848574\n",
            "600000 459606 441554 0.9607228800320274 30000 16462 0.5487333333333333 0.9635187530188901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmQ8GXt5AV87"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_path\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNcYyLEv7g4_",
        "outputId": "1fcba3b4-3270-4cb2-b4d7-87ad6ff7a8a9"
      },
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model_test.parameters() ) #if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2046389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxF2J5Qc-ATI"
      },
      "source": [
        "# layer 3\n",
        "0 458349 46074 0.10052165489615991 30000 0 0.0 0.5928713709422296\n",
        "60000 458349 381974 0.8333693321028299 30000 3174 0.1058 0.8352107237061714\n",
        "120000 458349 402954 0.8791423129536663 30000 7576 0.25253333333333333 0.8793386698781932\n",
        "180000 458349 414202 0.9036825650323226 30000 10038 0.3346 0.9014091881950217\n",
        "240000 458349 420016 0.916367222356763 30000 10856 0.36186666666666667 0.9149338168077163\n",
        "300000 458349 425010 0.9272628499244026 30000 11611 0.38703333333333334 0.9269508605887653\n",
        "360000 458349 427463 0.9326146669895647 30000 11988 0.3996 0.9335942698685936\n",
        "420000 458349 430899 0.9401111380192823 30000 12691 0.4230333333333333 0.9400827753524061\n",
        "480000 458349 431674 0.9418019893138199 30000 12825 0.4275 0.9418718051092072\n",
        "540000 458349 432263 0.9430870362976683 30000 12800 0.4266666666666667 0.9435102945572043\n",
        "600000 458349 432712 0.9440666391766972 30000 13187 0.43956666666666666 0.9449153374393748\n",
        "660000 458349 434135 0.9471712603278288 30000 13630 0.4543333333333333 0.9478519643328556\n",
        "720000 458349 433858 0.946566917349007 30000 13530 0.451 0.9474112521244729\n",
        "780000 458349 435215 0.9495275434221521 30000 13983 0.4661 0.949302823830749\n",
        "---------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb5IUsfXQPyG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}