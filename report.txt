I was not able to achieve 70%. My best run with one epoch is 56. In the first attempt I implemented basic LSTM and found that the performance converges to around 50. I did a visual inspection and found that the linear term of the calculation is particularly hard for the model to capture. After training on about 10% of the data during the first epoch, my model was able to predict non-number tokens correctly with a large accuracy score. However, it takes it a long time to be able to perform decently well on number calculations. I assumed that attention mechanism could help in a way that it establish more connection between tokens and numbers. However, my seq2seq with attention implementation and transformer did not do well enough. By the time I wrote this report, I was not able to figure out a viable solution due to limited computing power and limited time during final seasons. I would like to ask what may have been wrong in my implementation (especially on transformer) to make it not work as expected. Thank you for your time!
